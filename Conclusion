Conclusion

Looking at how I started this project and thinking about how hard this is going to be, but you got me intrigued in it and once I looked at it,
I knew it was the one. Starting with just trying this data set in everything that we have built over the semester and starting with some visualization 
of the data set and when I did the whole data set it broke my kernel repeatedly but would take its till to do it first. So, I sized it down big time going
from 1.19 million to 3100 why 3100 that’s how long my patience was with excels selecting though this large of a .csv file. After talking to you and I came 
back to boxplot. After getting the features, having a boxplot for each label, and playing with logs to scale the outliers. Individually I could log scale a 
negative number set but once I put it into a loop for the subplot of all the boxplots, I was getting a warning saying you can’t log scale a negative number, 
this would still show the subplot and boxplots on these features that had the negative data. So, I put in an if statement to set yscale to log and an else to 
scale linear witch is the default. Worked out pretty well and played with a few different logs 2,10, and e but it would only really change one or two labels in 
one or two features it seemed hard to see direct changes. Linear regression was also useless as I have all these features and it limits you on what you can visualize. 
Scatter plots are where things somewhat came together, finding some features that you could see some groups of classes but still just not very good. Until you use Kmeans 
and PCA, this is where you get the groups, and the centroids have some very solid classes identified so I started down this path and using the metrics you find there 
where a few classes that didn’t have the best scores. Still in this model there is one class recon_OSScan that couldn’t be grouped together no matter what features 
used to get there, it seems to cross over into at least 8 different classes. Also, having issues with Benign Traffic, Recon-Host Discovery, XSS, Dos-HTTP Flood,
MITM -Arp Spoofing, Command line Injection, Vulnerability scan, DDoS-Slow Loris, Backdoor malware, and browser hijacking. So too many classes have issues. It’s working
and putting in predictions but nothing to brag about. So next up was DTC and boy howdy did that work out. The first one I did and got back the metrics I was shocked
and at that point I was still using most of the features from 35 to 40 which turns out most of the improvement from Kmeans/PCA model was all from class 39 IAT Interval 
arrival time which is how much time between two packets being sent forward or by flow with a mean max min attribute. This was with just playing with the features and 
seeing what worked best eventually looking at the data and finding other columns with multiple digit values this is where I found Srate and Rate. Srate is the sampling
rate by day or hour breakdown, and rate is the I believe is the packet per second. Once you put all three in and use the training data, we get 100 across the metrics. 
So, is this overfit? Well after trying to find something to change I didn’t like anything I found with other features or just two features, so I put it to some test data 
from a different part of the original data set that I dropped the labels of and put up a predictions data frame to see if this works. And great success! Building the metrics 
to show this on the test data since I still have the labels, I’m getting 98% on the metrics and looking thought the predictions data frame you can see that its still the recon 
classes that have a few issues, but overall, I’m very happy with 98%. This turned-out way better than I ever expected it to. Learning way more about what kMeans/PCA is doing 
but more so what you can do with a DTC and if you get a DTC visualized with the rage in each leaf it’s a very good visual of how the tree chooses its label by range.
